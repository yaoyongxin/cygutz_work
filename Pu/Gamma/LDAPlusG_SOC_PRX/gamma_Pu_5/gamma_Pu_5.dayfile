Calculating gamma_Pu_5 in /scratch/ykent33700/gamma_Pu_5
on node018 with PID 20137




   start        Wed Jan 29 22:46:30 2014 with lapw0 (1/100 to go)

   cycle 0 	Wed Jan 29 22:46:30 2014 1000/0 to go

>lapw0      ( 22:46:30 ) starting parallel lapw0 at Wed Jan 29 22:46:30 CST 2014
-------- .machine0 : 2 processors
1.706u 0.130s 0:09.68 18.9%	0+0k 0+0io 13pf+0w
>lapw1      ( 22:46:40 ) starting parallel lapw1 at Wed Jan 29 22:46:40 CST 2014
->  starting parallel LAPW1 jobs at Wed Jan 29 22:46:40 CST 2014
running LAPW1 in parallel mode (using .machines)
2 number_of_parallel_jobs
[1] 20504
[2] 20523
[1]  - Done                          ( cd $PWD; $t $exe ${def}_$loop.def; rm -f .lock_$lockfile[$p] ) >>  ...
[3] 21351
[3]  + Done                          ( cd $PWD; $t $exe ${def}_$loop.def; rm -f .lock_$lockfile[$p] ) >>  ...
[2]  + Done                          ( cd $PWD; $t $exe ${def}_$loop.def; rm -f .lock_$lockfile[$p] ) >>  ...
     node018(665) 55.919u 0.901s 1:29.94 63.1%	0+0k 0+0io 0pf+0w
     node018(665) 55.757u 0.897s 1:31.98 61.5%	0+0k 0+0io 0pf+0w
     node018(1) 0.326u 0.011s 0:00.35 94.2%	0+0k 0+0io 0pf+0w
   Summary of lapw1para:
   node018	 k=1331	 user=112.002	 wallclock=182.27
112.169u 2.051s 1:34.27 121.1%	0+0k 0+0io 0pf+0w
>lapwso     ( 22:48:14 ) running LAPWSO in parallel mode
[1] 21497
[2] 21506
[2]  - Done                          ( cd $PWD; $t $exe ${def}_${loop}.def; rm -f .lock_$lockfile[$p] ) >>  ...
[1]  + Done                          ( cd $PWD; $t $exe ${def}_${loop}.def; rm -f .lock_$lockfile[$p] ) >>  ...
[1] 23106
[1]  + Done                          ( cd $PWD; $t $exe ${def}_${loop}.def; rm -f .lock_$lockfile[$p] ) >>  ...
      node018 37.163u 2.239s 2:56.15 22.3% 0+0k 0+0io 0pf+0w
      node018 37.313u 2.111s 2:55.29 22.4% 0+0k 0+0io 0pf+0w
      node018 0.066u 0.012s 0:00.13 53.8% 0+0k 0+0io 0pf+0w
   Summary of lapwsopara:
   node018	 user=74.542	 wallclock=351.57
74.659u 4.706s 2:58.56 44.4%	0+0k 0+0io 0pf+0w
>dmft1      ( 22:51:13 ) 22.96user 0.94system 0:34.35elapsed 69%CPU (0avgtext+0avgdata 0maxresident)k
>FastGutz   ( 22:51:47 ) 5914.11user 10.04system 1:38:21elapsed 100%CPU (0avgtext+0avgdata 0maxresident)k
>dmft2      ( 00:30:09 ) 420.42user 1.46system 3:34.36elapsed 196%CPU (0avgtext+0avgdata 0maxresident)k
>lcore      ( 00:33:43 ) 0.017u 0.003s 0:00.09 11.1%	0+0k 0+0io 0pf+0w
>mixer      ( 00:33:48 ) 0.030u 0.013s 0:01.06 3.7%	0+0k 0+0io 0pf+0w
:ENERGY convergence:  1.0
:CHARGE convergence:  0.0091916
>lapw0      ( 00:33:53 ) starting parallel lapw0 at Thu Jan 30 00:33:56 CST 2014
-------- .machine0 : 2 processors
3.933u 0.092s 0:26.26 15.3%	0+0k 0+0io 13pf+0w
>lapw1      ( 00:34:22 ) starting parallel lapw1 at Thu Jan 30 00:34:26 CST 2014
->  starting parallel LAPW1 jobs at Thu Jan 30 00:34:26 CST 2014
running LAPW1 in parallel mode (using .machines)
2 number_of_parallel_jobs
[1] 26908
[2] 26942
[1]  - Done                          ( cd $PWD; $t $exe ${def}_$loop.def; rm -f .lock_$lockfile[$p] ) >>  ...
[3] 27689
[3]  - Done                          ( cd $PWD; $t $exe ${def}_$loop.def; rm -f .lock_$lockfile[$p] ) >>  ...
[2]    Done                          ( cd $PWD; $t $exe ${def}_$loop.def; rm -f .lock_$lockfile[$p] ) >>  ...
     node018(665) 56.167u 0.986s 2:37.21 36.3%	0+0k 0+0io 0pf+0w
     node018(665) 56.413u 0.934s 2:41.77 35.4%	0+0k 0+0io 0pf+0w
     node018(1) 0.334u 0.006s 0:00.34 97.0%	0+0k 0+0io 0pf+0w
   Summary of lapw1para:
   node018	 k=1331	 user=112.914	 wallclock=319.32
113.133u 2.222s 2:49.58 68.0%	0+0k 0+0io 0pf+0w
>lapwso     ( 00:37:14 ) running LAPWSO in parallel mode
[1] 27857
[2] 27866
[1]  - Done                          ( cd $PWD; $t $exe ${def}_${loop}.def; rm -f .lock_$lockfile[$p] ) >>  ...
[3] 28394
[3]  - Done                          ( cd $PWD; $t $exe ${def}_${loop}.def; rm -f .lock_$lockfile[$p] ) >>  ...
[2]    Done                          ( cd $PWD; $t $exe ${def}_${loop}.def; rm -f .lock_$lockfile[$p] ) >>  ...
      node018 38.901u 2.094s 2:05.97 32.5% 0+0k 0+0io 0pf+0w
      node018 39.103u 2.073s 2:15.24 30.4% 0+0k 0+0io 0pf+0w
      node018 0.068u 0.013s 0:00.08 87.5% 0+0k 0+0io 0pf+0w
   Summary of lapwsopara:
   node018	 user=78.072	 wallclock=261.29
78.193u 4.732s 2:16.64 60.6%	0+0k 0+0io 0pf+0w
>dmft1      ( 00:39:31 ) 24.29user 1.21system 0:15.50elapsed 164%CPU (0avgtext+0avgdata 0maxresident)k
>FastGutz   ( 00:39:46 ) 2564.82user 8.19system 42:37.24elapsed 100%CPU (0avgtext+0avgdata 0maxresident)k
>dmft2      ( 01:22:23 ) 444.06user 1.60system 3:43.96elapsed 198%CPU (0avgtext+0avgdata 0maxresident)k
>lcore      ( 01:26:07 ) 0.012u 0.002s 0:00.01 100.0%	0+0k 0+0io 0pf+0w
>mixer      ( 01:26:07 ) 0.020u 0.014s 0:00.03 100.0%	0+0k 0+0io 0pf+0w
:ENERGY convergence:  3.22400010191e-05
:CHARGE convergence:  0.008298
>lapw0      ( 01:26:08 ) starting parallel lapw0 at Thu Jan 30 01:26:08 CST 2014
-------- .machine0 : 2 processors
1.942u 0.115s 0:03.11 65.9%	0+0k 0+0io 13pf+0w
>lapw1      ( 01:26:11 ) starting parallel lapw1 at Thu Jan 30 01:26:11 CST 2014
->  starting parallel LAPW1 jobs at Thu Jan 30 01:26:11 CST 2014
running LAPW1 in parallel mode (using .machines)
2 number_of_parallel_jobs
[1] 31593
[2] 31612
[1]  - Done                          ( cd $PWD; $t $exe ${def}_$loop.def; rm -f .lock_$lockfile[$p] ) >>  ...
[2]  + Done                          ( cd $PWD; $t $exe ${def}_$loop.def; rm -f .lock_$lockfile[$p] ) >>  ...
[1] 31704
[1]  + Done                          ( cd $PWD; $t $exe ${def}_$loop.def; rm -f .lock_$lockfile[$p] ) >>  ...
     node018(665) 58.987u 1.021s 1:01.38 97.7%	0+0k 0+0io 0pf+0w
     node018(665) 59.678u 1.040s 1:01.52 98.6%	0+0k 0+0io 0pf+0w
     node018(1) 0.329u 0.014s 0:00.34 97.0%	0+0k 0+0io 0pf+0w
   Summary of lapw1para:
   node018	 k=1331	 user=118.994	 wallclock=123.24
119.110u 2.299s 1:05.39 185.6%	0+0k 0+0io 0pf+0w
>lapwso     ( 01:27:16 ) running LAPWSO in parallel mode
[1] 31845
[2] 31851
[1]  - Done                          ( cd $PWD; $t $exe ${def}_${loop}.def; rm -f .lock_$lockfile[$p] ) >>  ...
[3] 32078
[3]  + Done                          ( cd $PWD; $t $exe ${def}_${loop}.def; rm -f .lock_$lockfile[$p] ) >>  ...
[2]  + Done                          ( cd $PWD; $t $exe ${def}_${loop}.def; rm -f .lock_$lockfile[$p] ) >>  ...
      node018 38.535u 1.989s 1:27.51 46.2% 0+0k 0+0io 0pf+0w
      node018 39.047u 1.988s 1:28.29 46.4% 0+0k 0+0io 0pf+0w
      node018 0.072u 0.009s 0:00.08 87.5% 0+0k 0+0io 0pf+0w
   Summary of lapwsopara:
   node018	 user=77.654	 wallclock=175.88
77.754u 4.533s 1:31.63 89.7%	0+0k 0+0io 0pf+0w
>dmft1      ( 01:28:48 ) 24.52user 1.35system 0:16.00elapsed 161%CPU (0avgtext+0avgdata 0maxresident)k
>FastGutz   ( 01:29:04 ) 2951.16user 9.34system 49:01.28elapsed 100%CPU (0avgtext+0avgdata 0maxresident)k
>dmft2      ( 02:18:05 ) 465.13user 1.80system 3:57.58elapsed 196%CPU (0avgtext+0avgdata 0maxresident)k
>lcore      ( 02:22:03 ) 0.012u 0.002s 0:00.01 100.0%	0+0k 0+0io 0pf+0w
>mixer      ( 02:22:03 ) 0.023u 0.016s 0:00.09 33.3%	0+0k 0+0io 0pf+0w
:ENERGY convergence:  0.000367850007024
:CHARGE convergence:  0.0020595
>lapw0      ( 02:22:03 ) starting parallel lapw0 at Thu Jan 30 02:22:03 CST 2014
-------- .machine0 : 2 processors
2.218u 0.110s 0:03.27 70.9%	0+0k 0+0io 13pf+0w
>lapw1      ( 02:22:06 ) starting parallel lapw1 at Thu Jan 30 02:22:06 CST 2014
->  starting parallel LAPW1 jobs at Thu Jan 30 02:22:06 CST 2014
running LAPW1 in parallel mode (using .machines)
2 number_of_parallel_jobs
[1] 2237
[2] 2256
[2]  - Done                          ( cd $PWD; $t $exe ${def}_$loop.def; rm -f .lock_$lockfile[$p] ) >>  ...
[1]  + Done                          ( cd $PWD; $t $exe ${def}_$loop.def; rm -f .lock_$lockfile[$p] ) >>  ...
[1] 2350
[1]  + Done                          ( cd $PWD; $t $exe ${def}_$loop.def; rm -f .lock_$lockfile[$p] ) >>  ...
     node018(665) 60.446u 1.035s 1:03.27 97.1%	0+0k 0+0io 0pf+0w
     node018(665) 60.814u 1.023s 1:03.09 98.0%	0+0k 0+0io 0pf+0w
     node018(1) 0.338u 0.014s 0:01.14 29.8%	0+0k 0+0io 0pf+0w
   Summary of lapw1para:
   node018	 k=1331	 user=121.598	 wallclock=127.5
121.722u 2.318s 1:07.41 183.9%	0+0k 0+0io 0pf+0w
>lapwso     ( 02:23:14 ) running LAPWSO in parallel mode
[1] 2501
[2] 2507
[1]  - Done                          ( cd $PWD; $t $exe ${def}_${loop}.def; rm -f .lock_$lockfile[$p] ) >>  ...
[3] 2703
[3]  - Done                          ( cd $PWD; $t $exe ${def}_${loop}.def; rm -f .lock_$lockfile[$p] ) >>  ...
[2]    Done                          ( cd $PWD; $t $exe ${def}_${loop}.def; rm -f .lock_$lockfile[$p] ) >>  ...
      node018 38.718u 2.055s 1:07.26 60.6% 0+0k 0+0io 0pf+0w
      node018 40.004u 2.087s 1:14.18 56.7% 0+0k 0+0io 0pf+0w
      node018 0.074u 0.011s 0:00.08 100.0% 0+0k 0+0io 0pf+0w
   Summary of lapwsopara:
   node018	 user=78.796	 wallclock=141.52
78.888u 4.647s 1:15.56 110.5%	0+0k 0+0io 0pf+0w
>dmft1      ( 02:24:29 ) 24.18user 1.28system 0:15.58elapsed 163%CPU (0avgtext+0avgdata 0maxresident)k
>FastGutz   ( 02:24:45 ) 2328.79user 9.09system 38:38.38elapsed 100%CPU (0avgtext+0avgdata 0maxresident)k
>dmft2      ( 03:03:23 ) 477.51user 1.79system 4:00.75elapsed 199%CPU (0avgtext+0avgdata 0maxresident)k
>lcore      ( 03:07:24 ) 0.013u 0.001s 0:00.01 100.0%	0+0k 0+0io 0pf+0w
>mixer      ( 03:07:24 ) 0.022u 0.018s 0:00.08 37.5%	0+0k 0+0io 0pf+0w
:ENERGY convergence:  3.55200027116e-05
:CHARGE convergence:  0.0011452
>lapw0      ( 03:07:24 ) starting parallel lapw0 at Thu Jan 30 03:07:25 CST 2014
-------- .machine0 : 2 processors
2.146u 0.105s 0:03.23 69.3%	0+0k 0+0io 13pf+0w
>lapw1      ( 03:07:28 ) starting parallel lapw1 at Thu Jan 30 03:07:28 CST 2014
->  starting parallel LAPW1 jobs at Thu Jan 30 03:07:28 CST 2014
running LAPW1 in parallel mode (using .machines)
2 number_of_parallel_jobs
[1] 5118
[2] 5137
[2]  - Done                          ( cd $PWD; $t $exe ${def}_$loop.def; rm -f .lock_$lockfile[$p] ) >>  ...
[2] 5227
[2]  - Done                          ( cd $PWD; $t $exe ${def}_$loop.def; rm -f .lock_$lockfile[$p] ) >>  ...
[1]  + Done                          ( cd $PWD; $t $exe ${def}_$loop.def; rm -f .lock_$lockfile[$p] ) >>  ...
     node018(665) 59.559u 1.004s 1:01.35 98.6%	0+0k 0+0io 0pf+0w
     node018(665) 58.693u 1.025s 0:59.84 99.7%	0+0k 0+0io 0pf+0w
     node018(1) 0.337u 0.015s 0:00.35 97.1%	0+0k 0+0io 0pf+0w
   Summary of lapw1para:
   node018	 k=1331	 user=118.589	 wallclock=121.54
118.706u 2.276s 1:04.39 187.8%	0+0k 0+0io 0pf+0w
>lapwso     ( 03:08:32 ) running LAPWSO in parallel mode
[1] 5369
[2] 5375
[1]  - Done                          ( cd $PWD; $t $exe ${def}_${loop}.def; rm -f .lock_$lockfile[$p] ) >>  ...
[3] 5556
[3]  - Done                          ( cd $PWD; $t $exe ${def}_${loop}.def; rm -f .lock_$lockfile[$p] ) >>  ...
[2]    Done                          ( cd $PWD; $t $exe ${def}_${loop}.def; rm -f .lock_$lockfile[$p] ) >>  ...
      node018 39.012u 2.050s 1:08.47 59.9% 0+0k 0+0io 0pf+0w
      node018 39.045u 1.973s 1:17.08 53.2% 0+0k 0+0io 0pf+0w
      node018 0.070u 0.010s 0:00.08 100.0% 0+0k 0+0io 0pf+0w
   Summary of lapwsopara:
   node018	 user=78.127	 wallclock=145.63
78.229u 4.503s 1:18.47 105.4%	0+0k 0+0io 0pf+0w
>dmft1      ( 03:09:51 ) 24.60user 1.44system 0:36.38elapsed 71%CPU (0avgtext+0avgdata 0maxresident)k
>FastGutz   ( 03:10:27 ) 2316.42user 9.03system 38:26.58elapsed 100%CPU (0avgtext+0avgdata 0maxresident)k
>dmft2      ( 03:48:54 ) 436.93user 1.59system 4:02.64elapsed 180%CPU (0avgtext+0avgdata 0maxresident)k
>lcore      ( 03:52:56 ) 0.012u 0.002s 0:00.01 100.0%	0+0k 0+0io 0pf+0w
>mixer      ( 03:52:56 ) 0.026u 0.012s 0:00.04 75.0%	0+0k 0+0io 0pf+0w
:ENERGY convergence:  0.000106200008304
:CHARGE convergence:  0.0010074
>lapw0      ( 03:52:57 ) starting parallel lapw0 at Thu Jan 30 03:52:58 CST 2014
-------- .machine0 : 2 processors
1.972u 0.110s 0:07.08 29.3%	0+0k 0+0io 13pf+0w
>lapw1      ( 03:53:04 ) starting parallel lapw1 at Thu Jan 30 03:53:04 CST 2014
->  starting parallel LAPW1 jobs at Thu Jan 30 03:53:04 CST 2014
running LAPW1 in parallel mode (using .machines)
2 number_of_parallel_jobs
[1] 8579
[2] 8614
[1]  - Done                          ( cd $PWD; $t $exe ${def}_$loop.def; rm -f .lock_$lockfile[$p] ) >>  ...
[3] 8825
[3]  - Done                          ( cd $PWD; $t $exe ${def}_$loop.def; rm -f .lock_$lockfile[$p] ) >>  ...
[2]  + Done                          ( cd $PWD; $t $exe ${def}_$loop.def; rm -f .lock_$lockfile[$p] ) >>  ...
     node018(665) 58.427u 1.044s 1:27.56 67.9%	0+0k 0+0io 0pf+0w
     node018(665) 58.377u 0.998s 1:23.82 70.8%	0+0k 0+0io 0pf+0w
     node018(1) 0.335u 0.013s 0:00.35 97.1%	0+0k 0+0io 0pf+0w
   Summary of lapw1para:
   node018	 k=1331	 user=117.139	 wallclock=171.73
117.275u 2.289s 1:32.04 129.8%	0+0k 0+0io 0pf+0w
>lapwso     ( 03:54:36 ) running LAPWSO in parallel mode
[1] 8968
[2] 8975
[1]  - Done                          ( cd $PWD; $t $exe ${def}_${loop}.def; rm -f .lock_$lockfile[$p] ) >>  ...
[3] 9156
[3]  + Done                          ( cd $PWD; $t $exe ${def}_${loop}.def; rm -f .lock_$lockfile[$p] ) >>  ...
[2]  + Done                          ( cd $PWD; $t $exe ${def}_${loop}.def; rm -f .lock_$lockfile[$p] ) >>  ...
      node018 37.172u 1.915s 1:07.40 57.9% 0+0k 0+0io 0pf+0w
      node018 39.276u 2.038s 1:22.37 50.1% 0+0k 0+0io 0pf+0w
      node018 0.067u 0.019s 0:08.07 0.8% 0+0k 0+0io 0pf+0w
   Summary of lapwsopara:
   node018	 user=76.515	 wallclock=157.84
76.608u 4.428s 1:24.02 96.4%	0+0k 0+0io 0pf+0w
>dmft1      ( 03:56:00 ) 24.20user 1.18system 0:14.06elapsed 180%CPU (0avgtext+0avgdata 0maxresident)k
>FastGutz   ( 03:56:14 ) 2596.24user 9.69system 43:08.21elapsed 100%CPU (0avgtext+0avgdata 0maxresident)k
>dmft2      ( 04:39:22 ) 455.83user 1.69system 3:49.90elapsed 199%CPU (0avgtext+0avgdata 0maxresident)k
>lcore      ( 04:43:12 ) 0.013u 0.000s 0:00.01 100.0%	0+0k 0+0io 0pf+0w
>mixer      ( 04:44:45 ) 0.027u 0.013s 0:00.08 37.5%	0+0k 0+0io 0pf+0w
:ENERGY convergence:  4.56300040241e-05
:CHARGE convergence:  0.000237
>lapw0      ( 04:44:45 ) starting parallel lapw0 at Thu Jan 30 04:44:46 CST 2014
-------- .machine0 : 2 processors
2.272u 0.110s 0:03.30 72.1%	0+0k 0+0io 13pf+0w
>lapw1      ( 04:44:49 ) starting parallel lapw1 at Thu Jan 30 04:44:49 CST 2014
->  starting parallel LAPW1 jobs at Thu Jan 30 04:44:49 CST 2014
running LAPW1 in parallel mode (using .machines)
2 number_of_parallel_jobs
[1] 12893
[2] 12913
[1]  - Done                          ( cd $PWD; $t $exe ${def}_$loop.def; rm -f .lock_$lockfile[$p] ) >>  ...
[3] 13005
[3]  + Done                          ( cd $PWD; $t $exe ${def}_$loop.def; rm -f .lock_$lockfile[$p] ) >>  ...
[2]  + Done                          ( cd $PWD; $t $exe ${def}_$loop.def; rm -f .lock_$lockfile[$p] ) >>  ...
     node018(665) 60.575u 0.973s 1:02.62 98.2%	0+0k 0+0io 0pf+0w
     node018(665) 60.218u 1.058s 1:02.67 97.7%	0+0k 0+0io 0pf+0w
     node018(1) 0.343u 0.015s 0:00.36 97.2%	0+0k 0+0io 0pf+0w
   Summary of lapw1para:
   node018	 k=1331	 user=121.136	 wallclock=125.65
121.252u 2.285s 1:05.40 188.8%	0+0k 0+0io 0pf+0w
>lapwso     ( 04:45:54 ) running LAPWSO in parallel mode
[1] 13146
[2] 13152
[1]  - Done                          ( cd $PWD; $t $exe ${def}_${loop}.def; rm -f .lock_$lockfile[$p] ) >>  ...
[3] 13350
[3]  - Done                          ( cd $PWD; $t $exe ${def}_${loop}.def; rm -f .lock_$lockfile[$p] ) >>  ...
[2]    Done                          ( cd $PWD; $t $exe ${def}_${loop}.def; rm -f .lock_$lockfile[$p] ) >>  ...
      node018 38.863u 2.015s 1:05.72 62.1% 0+0k 0+0io 0pf+0w
      node018 40.298u 2.072s 1:13.13 57.9% 0+0k 0+0io 0pf+0w
      node018 0.077u 0.007s 0:00.08 87.5% 0+0k 0+0io 0pf+0w
   Summary of lapwsopara:
   node018	 user=79.238	 wallclock=138.93
79.336u 4.585s 1:14.53 112.5%	0+0k 0+0io 0pf+0w
>dmft1      ( 04:47:09 ) 24.50user 1.28system 0:16.99elapsed 151%CPU (0avgtext+0avgdata 0maxresident)k
>FastGutz   ( 04:47:26 ) 2387.50user 9.22system 39:38.48elapsed 100%CPU (0avgtext+0avgdata 0maxresident)k
>dmft2      ( 05:27:04 ) 449.71user 1.66system 3:47.00elapsed 198%CPU (0avgtext+0avgdata 0maxresident)k
>lcore      ( 05:30:51 ) 0.011u 0.002s 0:00.01 100.0%	0+0k 0+0io 0pf+0w
>mixer      ( 05:31:12 ) 0.026u 0.011s 0:00.04 75.0%	0+0k 0+0io 0pf+0w
:ENERGY convergence:  1.23000063468e-06
:CHARGE convergence:  0.0001515
